version: "3.8"

x-deploy: &default-deploy
  replicas: 1
  restart_policy:
    condition: on-failure

x-deploy-manager: &default-deploy-manager
  <<: *default-deploy
  placement:
    constraints:
      - "node.role==manager"

x-deploy-worker: &default-deploy-worker
  <<: *default-deploy
  placement:
    constraints:
      - "node.role==worker"

services:
  visualizer:
    image: dockersamples/visualizer:latest
    volumes:
      - "/var/run/docker.sock:/var/run/docker.sock"
    ports:
      - "80:8080"
    deploy: *default-deploy-manager

  sparkmaster:
    image: jonatasmiguel/spark_image:latest
    ports:
      - "4040:4040"
      - "7077:7077"
      - "8080:8080"
    environment:
      - SPARK_ROLE=master
    deploy: *default-deploy-manager
    networks:
      - cluster_network
    volumes:
      - global-volume:/checkpoint

  sparkworker:
    image: jonatasmiguel/spark_image:latest
    entrypoint: dockerize -wait tcp://sparkmaster:7077 -timeout 240s /sbin/my_init
    ports:
      - "8081:8081"
      - "41352:41352"
    environment:
      - SPARK_MASTER=sparkmaster
      - SPARK_ROLE=slave
    deploy: *default-deploy-worker
    networks:
      - cluster_network
    volumes:
      - global-volume:/checkpoint

  streaming:
    image: jonatasmiguel/streaming_image:latest
    depends_on: 
      - sparkmaster
      - sparkworker
      - dashboards
      - twitter
      - namenode
      - datanode-0
    extra_hosts:
      - "streaming:0.0.0.0"
    ports:
      - "5005:5005"
      - "41100:41100"
    deploy: *default-deploy-worker
    networks:
      - cluster_network
    volumes:
      - global-volume:/checkpoint

  dashboards:
    image: jonatasmiguel/dashboards_image:latest
    extra_hosts:
      - "dashboards:0.0.0.0"
    ports:
      - "5009:5009"
    deploy: *default-deploy-worker
    networks:
      - cluster_network

  twitter:
    image: jonatasmiguel/twitter_image:latest
    extra_hosts:
      - "twitter:0.0.0.0"
    ports:
      - "9017:9017"
    deploy: *default-deploy-worker
    networks:
      - cluster_network

  primary-namenode:
    hostname: primary-namenode
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    ports:
      - "9876:50070"
      - "9870:9870"
      - "8020:8020"
    environment:
      - CLUSTER_NAME=DownunderHDFS
      - INIT_DAEMON_STEP=setup_hdfs
      - HDFS_CONF_dfs_webhdfs_enabled=true
      - HDFS_CONF_dfs_permissions_enabled=false
      # - HDFS_CONF_dfs_disk_balancer_enabled=false
      - HDFS_CONF_dfs_namenode_datanode_registration_iphostnamecheck=true
      - CORE_CONF_hadoop_http_staticuser_user=root
      - CORE_CONF_hadoop_proxyuser_hue_hosts=*
      - CORE_CONF_hadoop_proxyuser_hue_groups=*
      - CORE_CONF_io_compression_codecs=org.apache.hadoop.io.compress.SnappyCodec
      - HDFS_CONF_dfs_namenode_datanode_registration_ip___hostname___check=false
      - HDFS_CONF_dfs_namenode_datanode_registration_ip___io___check=false
      - HDFS_CONF_dfs_client_use_datanode_hostname=true
      - HDFS_CONF_dfs_datanode_use_datanode_hostname=true
      - HDFS_CONF_dfs_disk_balancer_enabled=true
      - HDFS_CONF_dfs_namenode_rpc___bind___host=0.0.0.0
      - HDFS_CONF_dfs_namenode_servicerpc___bind___host=0.0.0.0
      - HDFS_CONF_dfs_namenode_http___bind___host=0.0.0.0
      - HDFS_CONF_dfs_namenode_https___bind___host=0.0.0.0
    volumes:
      - primary-namenode:/hadoop/dfs/name
    networks:
      - cluster_network
          # aliases:
          #   "primary-namenode"
    deploy:
      mode: replicated
      replicas: 1
      # replicas: 
      # update_config:
      #   parallelism: 2
      #   delay: 10s
      restart_policy:
        condition: on-failure
      placement:
        constraints:
          - node.role == manager

  secondary-namenode:
    hostname: secondary-namenode
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    ports:
      - "9877:50070"
      - "9871:9870"
      - "8021:8020"
    environment:
      - CLUSTER_NAME=DownunderHDFS
      # - INIT_DAEMON_STEP=setup_hdfs
      - HDFS_CONF_dfs_webhdfs_enabled=true
      - HDFS_CONF_dfs_permissions_enabled=false
      - HDFS_CONF_dfs_namenode_datanode_registration_iphostnamecheck=true
      - CORE_CONF_hadoop_http_staticuser_user=root
      - CORE_CONF_hadoop_proxyuser_hue_hosts=*
      - CORE_CONF_hadoop_proxyuser_hue_groups=*
      - CORE_CONF_io_compression_codecs=org.apache.hadoop.io.compress.SnappyCodec
      - HDFS_CONF_dfs_namenode_datanode_registration_ip___hostname___check=false
      - HDFS_CONF_dfs_namenode_datanode_registration_ip___io___check=false
      - HDFS_CONF_dfs_client_use_datanode_hostname=true
      - HDFS_CONF_dfs_datanode_use_datanode_hostname=true
      - HDFS_CONF_dfs_disk_balancer_enabled=true
      - HDFS_CONF_dfs_namenode_rpc___bind___host=0.0.0.0
      - HDFS_CONF_dfs_namenode_servicerpc___bind___host=0.0.0.0
      - HDFS_CONF_dfs_namenode_http___bind___host=0.0.0.0
      - HDFS_CONF_dfs_namenode_https___bind___host=0.0.0.0
    volumes:
      - secondary-namenode:/hadoop/dfs/name
    networks:
      - cluster_network
    deploy:
      mode: replicated
      replicas: 1
      # replicas: 
      # update_config:
      #   parallelism: 2
      #   delay: 10s
      restart_policy:
        condition: on-failure
      placement:
        constraints:
          - node.role == worker        
     
  datanode:
    hostname: datanode
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    environment:
      - CLUSTER_NAME=DownunderHDFS
      - CORE_CONF_fs_defaultFS=hdfs://primary-namenode:8020
      - HDFS_CONF_dfs_webhdfs_enabled=true
      - HDFS_CONF_dfs_namenode_datanode_registration_iphostnamecheck=true
      - CORE_CONF_hadoop_http_staticuser_user=root
      - CORE_CONF_hadoop_proxyuser_hue_hosts=*
      - CORE_CONF_hadoop_proxyuser_hue_groups=*
      - CORE_CONF_io_compression_codecs=org.apache.hadoop.io.compress.SnappyCodec
      - HDFS_CONF_dfs_permissions_enabled=false
      - HDFS_CONF_dfs_namenode_datanode_registration_ip___hostname___check=false
      - HDFS_CONF_dfs_namenode_datanode_registration_ip___io___check=false
      - HDFS_CONF_dfs_client_use_datanode_hostname=true
      - HDFS_CONF_dfs_datanode_use_datanode_hostname=true
      - HDFS_CONF_dfs_disk_balancer_enabled=true
      - HDFS_CONF_dfs_namenode_rpc___bind___host=0.0.0.0
      - HDFS_CONF_dfs_namenode_servicerpc___bind___host=0.0.0.0
      - HDFS_CONF_dfs_namenode_http___bind___host=0.0.0.0
      - HDFS_CONF_dfs_namenode_https___bind___host=0.0.0.0
    ports:
      - "9878:50075"
      - "9864:9864"
      - "50020:50020"
      - "41430:41430"
      - "50010:50010"
    links: 
      - primary-namenode
    volumes:
      - datanode:/hadoop/dfs/data
    networks:
      - cluster_network
          # aliases:
          #   - "datanode"
    depends_on: 
      - primary-namenode
      - secondary-namenode
    deploy:
      mode: global
      # replicas: 
      # update_config:
      #   parallelism: 2
      #   delay: 10s
      restart_policy:
        condition: on-failure
      placement:
        constraints:
          - node.role == worker
      
      
      
networks:
 cluster_network:
    attachable: true
    ipam:
       driver: default
    

volumes:
  global-volume:
  data-0:
  data-1:
  name:
  primary-namenode:
  secondary-namenode:
  datanode: